{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b931bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a434b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b32557",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c2bdccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix: African-American Names vs Unpleasant Words\n",
      "[[0.5894325  0.5653029  0.44624683 0.45218742 0.612097   0.49761528\n",
      "  0.51331174 0.5536804  0.5729841  0.63055515]\n",
      " [0.63213444 0.6081727  0.48557472 0.56075937 0.6258296  0.5105423\n",
      "  0.5020666  0.5431477  0.592612   0.54960203]\n",
      " [0.60569656 0.6078923  0.49174184 0.45942447 0.64932346 0.6464484\n",
      "  0.56112725 0.6547514  0.6576848  0.7170972 ]\n",
      " [0.7121114  0.68451405 0.50592834 0.6985651  0.73315537 0.5879741\n",
      "  0.6429105  0.6485186  0.7091766  0.62303096]\n",
      " [0.6490774  0.60346484 0.49587324 0.55428547 0.6072122  0.5366373\n",
      "  0.59237206 0.6037488  0.6614637  0.58573806]\n",
      " [0.65687454 0.6241708  0.47220105 0.5831802  0.67904305 0.56204027\n",
      "  0.5203844  0.56624335 0.6613837  0.5792254 ]\n",
      " [0.7230031  0.65412307 0.48936838 0.64919794 0.67122096 0.5832323\n",
      "  0.5435049  0.6316592  0.6888602  0.6058235 ]\n",
      " [0.6232985  0.6405214  0.5328785  0.54608077 0.64366865 0.52398425\n",
      "  0.5662793  0.5982258  0.6037876  0.56898683]\n",
      " [0.6156524  0.57451975 0.4494065  0.5625051  0.6356442  0.54229426\n",
      "  0.51220614 0.5385371  0.5835773  0.57562864]\n",
      " [0.64668703 0.60382116 0.43861386 0.4768226  0.5779784  0.5739755\n",
      "  0.51625896 0.60613763 0.63322896 0.636908  ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Example word sets\n",
    "set1_words = [\"Reginald\", \"Kameron\", \"Kendrick\", \"Javon\", \"Tyrell\", \"Jamar\", \"Camron\", \"Tyree\", \"Jamari\", \"Reggie\"]\n",
    "set2_words = [\"rude\", \"lazy\", \"disagreeable\", \"lousy\", \"sad\", \"hate\", \"violent\", \"bitter\", \"harsh\", \"angry\"]\n",
    "\n",
    "# Tokenize words and get embeddings\n",
    "def get_word_embeddings(word_list):\n",
    "    tokens = tokenizer(word_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pooling over tokens\n",
    "    return embeddings\n",
    "\n",
    "# Calculate cosine similarity\n",
    "def calculate_cosine_similarity(embeddings_set1, embeddings_set2):\n",
    "    similarities = cosine_similarity(embeddings_set1, embeddings_set2)\n",
    "    return similarities\n",
    "\n",
    "# Get embeddings for the word sets\n",
    "embeddings_set1 = get_word_embeddings(set1_words)\n",
    "embeddings_set2 = get_word_embeddings(set2_words)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarities = calculate_cosine_similarity(embeddings_set1, embeddings_set2)\n",
    "\n",
    "# Print the cosine similarity matrix\n",
    "print(\"Cosine Similarity Matrix: African-American Names vs Unpleasant Words\")\n",
    "print(similarities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23712c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix: Afircan-American Names vs Pleasant Words\n",
      "[[0.591162   0.43056116 0.5368997  0.47021532 0.5333515  0.5549301\n",
      "  0.5878211  0.49887764 0.4868008  0.54899555]\n",
      " [0.51445615 0.43632767 0.6233879  0.5435183  0.58648956 0.59047943\n",
      "  0.62446    0.4692996  0.5331775  0.57225704]\n",
      " [0.70975065 0.5186647  0.56972206 0.51795673 0.54228365 0.72729456\n",
      "  0.6159319  0.5626155  0.565778   0.72731745]\n",
      " [0.6513032  0.4561221  0.6028489  0.48592347 0.73876786 0.6787504\n",
      "  0.71213734 0.5008937  0.5553807  0.6586697 ]\n",
      " [0.5926456  0.42797193 0.60487187 0.50713277 0.63332283 0.6565965\n",
      "  0.59900916 0.5114886  0.5270775  0.6340762 ]\n",
      " [0.5487418  0.42113397 0.5518431  0.49849746 0.6236964  0.565477\n",
      "  0.6348306  0.4726201  0.5440173  0.5928569 ]\n",
      " [0.58033645 0.44364282 0.6317308  0.53753686 0.6572124  0.6474309\n",
      "  0.6801305  0.5116414  0.5721454  0.66827565]\n",
      " [0.53736365 0.43341485 0.5766705  0.4732828  0.62947416 0.5895819\n",
      "  0.5615423  0.4979097  0.5259892  0.5895657 ]\n",
      " [0.5526614  0.42226094 0.53685653 0.4774515  0.6049593  0.51924694\n",
      "  0.6478884  0.45853725 0.525887   0.531694  ]\n",
      " [0.6921854  0.39168215 0.51567423 0.4060386  0.5298883  0.61103576\n",
      "  0.58987284 0.46324033 0.5237231  0.6386312 ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
    "\n",
    "# Example word sets\n",
    "set1_words = [\"Reginald\", \"Kameron\", \"Kendrick\", \"Javon\", \"Tyrell\", \"Jamar\", \"Camron\", \"Tyree\", \"Jamari\", \"Reggie\"]\n",
    "set2_words = [\"Happy\", \"Agreeable\", \"Polite\", \"Civil\", \"Charming\", \"Gracious\", \"Gentle\", \"Approachable\", \"Love\", \"Cool\"]\n",
    "\n",
    "# Tokenize words and get embeddings\n",
    "def get_word_embeddings(word_list):\n",
    "    tokens = tokenizer(word_list, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # Average pooling over tokens\n",
    "    return embeddings\n",
    "\n",
    "# Calculate cosine similarity\n",
    "def calculate_cosine_similarity(embeddings_set1, embeddings_set2):\n",
    "    similarities = cosine_similarity(embeddings_set1, embeddings_set2)\n",
    "    return similarities\n",
    "\n",
    "# Get embeddings for the word sets\n",
    "embeddings_set1 = get_word_embeddings(set1_words)\n",
    "embeddings_set2 = get_word_embeddings(set2_words)\n",
    "\n",
    "# Normalize embeddings\n",
    "embeddings_set1_normalized = torch.nn.functional.normalize(embeddings_set1, p=2, dim=1)\n",
    "embeddings_set2_normalized = torch.nn.functional.normalize(embeddings_set2, p=2, dim=1)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarities = calculate_cosine_similarity(embeddings_set1, embeddings_set2)\n",
    "\n",
    "# Print the cosine similarity matrix\n",
    "print(\"Cosine Similarity Matrix: Afircan-American Names vs Pleasant Words\")\n",
    "print(similarities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
